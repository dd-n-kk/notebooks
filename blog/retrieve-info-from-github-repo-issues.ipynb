{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "date:\n",
        "  created: 2025-04-22\n",
        "  updated: 2025-04-22\n",
        "\n",
        "categories:\n",
        "- Deep learning\n",
        "\n",
        "tags:\n",
        "- NLP\n",
        "- Polars\n",
        "- Hugging Face\n",
        "- \"Series: GitHub repo issues dataset\"\n",
        "\n",
        "slug: retrieve-info-from-github-repo-issues\n",
        "---\n",
        "\n",
        "# Retrieving information from a GitHub repo issues dataset\n",
        "\n",
        "This is Part III of my adaptation of\n",
        ":simple-huggingface: Hugging Face NLP Course: [Creating your own dataset][1].\n",
        "It consists of several parts:\n",
        "\n",
        "1. Creating a corpus of issue-comment pairs from the previously prepared dataset.\n",
        "2. Embedding each issue-comment pair into dense vectors for similarity search.\n",
        "3. Building a Faiss index of the embeddings to speed up querying.\n",
        "4. Using a reranker to pick the best entries from the top-k similarity search results.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/dd-n-kk/notebooks/blob/main/blog/retrieve-info-from-github-repo-issues.ipynb\" target=\"_parent\">\n",
        "    :simple-googlecolab: Colab notebook\n",
        "</a>\n",
        "\n",
        "<!-- more -->"
      ],
      "metadata": {
        "id": "T997u5VH4kwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparations"
      ],
      "metadata": {
        "id": "1OFSziMBSdoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install -Uq polars\n",
        "!uv pip install -q datasets faiss-cpu"
      ],
      "metadata": {
        "id": "dIW7uwVpc4vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Sequence\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import torch as tc\n",
        "from datasets import load_dataset\n",
        "from numpy.typing import NDArray\n",
        "from polars import col\n",
        "from torch.nn import functional as F\n",
        "from tqdm.auto import trange\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "i5c9tVR7xcy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = pl.Config(\n",
        "    float_precision=3,\n",
        "    fmt_str_lengths=200,\n",
        "    fmt_table_cell_list_len=-1,\n",
        "    tbl_cols=-1,\n",
        "    tbl_rows=100,\n",
        "    tbl_width_chars=-1,\n",
        ")"
      ],
      "metadata": {
        "id": "Wa_dCyqjzQ6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the corpus\n",
        "\n",
        "We directly download the `dd-n-kk/uv-github-issues` dataset prepared in [Part II][2]."
      ],
      "metadata": {
        "id": "5inseUhRVFgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "issues = load_dataset(\"dd-n-kk/uv-github-issues\", \"issues\")\n",
        "comments = load_dataset(\"dd-n-kk/uv-github-issues\", \"comments\")"
      ],
      "metadata": {
        "id": "Fm0t2Rvewvv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train-test splits are merged because all processed entries will be used for querying."
      ],
      "metadata": {
        "id": "w2zrGEmiVMpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "issues_df = pl.concat([issues[\"train\"].to_polars(), issues[\"test\"].to_polars()])\n",
        "comments_df = pl.concat([comments[\"train\"].to_polars(), comments[\"test\"].to_polars()])"
      ],
      "metadata": {
        "id": "qdip6hV7xU4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Issues\n",
        "\n",
        "I decide to remove:\n",
        "\n",
        "- Issues with null bodies.\n",
        "- Issues created by bots, because they usually contain little info.\n",
        "- Pull requests not yet merged, for they often contain suggestions not yet adopted."
      ],
      "metadata": {
        "id": "IjqHYPN8Vyyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "issues_df = issues_df.filter(\n",
        "    col(\"body\").is_not_null()\n",
        "    & (~col(\"user\").str.contains(\"[bot]\", literal=True))\n",
        "    & (~col(\"pull_request\") | col(\"merged_at\").is_not_null())\n",
        ")"
      ],
      "metadata": {
        "id": "ITgM3BiX3y2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A crude word count reveals a problem: A small number of issues are extremely long."
      ],
      "metadata": {
        "id": "JUOt9Ws5Wkct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = (\n",
        "    issues_df.select(\n",
        "        \"html_url\", \"title\", col(\"body\").str.split(\" \").list.len().alias(\"n_words\")\n",
        "    )\n",
        "    .sort(\"n_words\", descending=True)\n",
        ")\n",
        "q.get_column(\"n_words\").describe()"
      ],
      "metadata": {
        "id": "mbVfbnU43R53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "4a960404-c009-41ad-e87f-3a61662ff766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (9, 2)\n",
              "┌────────────┬───────────┐\n",
              "│ statistic  ┆ value     │\n",
              "│ ---        ┆ ---       │\n",
              "│ str        ┆ f64       │\n",
              "╞════════════╪═══════════╡\n",
              "│ count      ┆ 10313.000 │\n",
              "│ null_count ┆ 0.000     │\n",
              "│ mean       ┆ 152.041   │\n",
              "│ std        ┆ 424.822   │\n",
              "│ min        ┆ 1.000     │\n",
              "│ 25%        ┆ 21.000    │\n",
              "│ 50%        ┆ 62.000    │\n",
              "│ 75%        ┆ 151.000   │\n",
              "│ max        ┆ 12590.000 │\n",
              "└────────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>value</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>10313.000</td></tr><tr><td>&quot;null_count&quot;</td><td>0.000</td></tr><tr><td>&quot;mean&quot;</td><td>152.041</td></tr><tr><td>&quot;std&quot;</td><td>424.822</td></tr><tr><td>&quot;min&quot;</td><td>1.000</td></tr><tr><td>&quot;25%&quot;</td><td>21.000</td></tr><tr><td>&quot;50%&quot;</td><td>62.000</td></tr><tr><td>&quot;75%&quot;</td><td>151.000</td></tr><tr><td>&quot;max&quot;</td><td>12590.000</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the GitHub web pages we can see that they contain long debug outputs,\n",
        "which are unlikely to help answer user questions."
      ],
      "metadata": {
        "id": "10VUlVYUwfri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "eYVBPaAX4_TP",
        "outputId": "70a59e39-6564-4c7f-b1a3-39175708a931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 3)\n",
              "┌─────────────────────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────────────────┬─────────┐\n",
              "│ html_url                                    ┆ title                                                                                           ┆ n_words │\n",
              "│ ---                                         ┆ ---                                                                                             ┆ ---     │\n",
              "│ str                                         ┆ str                                                                                             ┆ u32     │\n",
              "╞═════════════════════════════════════════════╪═════════════════════════════════════════════════════════════════════════════════════════════════╪═════════╡\n",
              "│ https://github.com/astral-sh/uv/issues/6443 ┆ `uv sync` freezes infinitely at the container root                                              ┆ 12590   │\n",
              "│ https://github.com/astral-sh/uv/issues/5742 ┆ Allow `uv sync --no-build-isolation`                                                            ┆ 11862   │\n",
              "│ https://github.com/astral-sh/uv/issues/5046 ┆ Bad resolver error for `colabfold[alphafold]==1.5.5` on python 3.11                             ┆ 11764   │\n",
              "│ https://github.com/astral-sh/uv/issues/7183 ┆ Improve Python version resolution UI                                                            ┆ 11316   │\n",
              "│ https://github.com/astral-sh/uv/issues/2062 ┆ uv pip and python -m pip resolve different versions of tensorflow in pyhf developer environment ┆ 8819    │\n",
              "└─────────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>html_url</th><th>title</th><th>n_words</th></tr><tr><td>str</td><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;https://github.com/astral-sh/uv/issues/6443&quot;</td><td>&quot;`uv sync` freezes infinitely at the container root&quot;</td><td>12590</td></tr><tr><td>&quot;https://github.com/astral-sh/uv/issues/5742&quot;</td><td>&quot;Allow `uv sync --no-build-isolation`&quot;</td><td>11862</td></tr><tr><td>&quot;https://github.com/astral-sh/uv/issues/5046&quot;</td><td>&quot;Bad resolver error for `colabfold[alphafold]==1.5.5` on python 3.11&quot;</td><td>11764</td></tr><tr><td>&quot;https://github.com/astral-sh/uv/issues/7183&quot;</td><td>&quot;Improve Python version resolution UI&quot;</td><td>11316</td></tr><tr><td>&quot;https://github.com/astral-sh/uv/issues/2062&quot;</td><td>&quot;uv pip and python -m pip resolve different versions of tensorflow in pyhf developer environment&quot;</td><td>8819</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To shorten these issues, I:\n",
        "\n",
        "- Replace Markdown fenced code blocks containing too many characters with `[CODE]`.\n",
        "- Replace each HTML element `<detail>` with `[DETAIL]`.\n",
        "- Replace HTML comments `<!-- ... -->` with `[COMMENT]`.\n",
        "- Remove trailing whitespaces."
      ],
      "metadata": {
        "id": "VmMtgL5qXJTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "issues_df = (\n",
        "    issues_df.lazy()\n",
        "    .select(\n",
        "        \"number\",\n",
        "        col(\"html_url\").alias(\"issue_url\"),\n",
        "        \"title\",\n",
        "        (\n",
        "            col(\"body\").str.replace_all(r\"\\s*[\\r\\n]\", \"\\n\")\n",
        "            .str.replace_all(r\"(?s)<details>.*?</details>\", \"[DETAILS]\")\n",
        "            .str.replace_all(r\"(?s)<!--.*?-->\", \"[COMMENT]\")\n",
        "            .str.replace_all(r\"```(?:[^`]|`[^`]|``[^`]){768,}```\", \"[CODE]\")\n",
        "            .str.replace_all(r\"~~~(?:[^~]|~[^~]|~~[^~]){768,}~~~\", \"[CODE]\")\n",
        "        ),\n",
        "    )\n",
        "    .collect()\n",
        ")"
      ],
      "metadata": {
        "id": "UGg7nbEB3Jf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An issue body containing long debug outputs now looks like this:"
      ],
      "metadata": {
        "id": "9lkDrcvBxRam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(issues_df.filter(col(\"number\") == 6443).item(0, \"body\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONOb3gAHvcuo",
        "outputId": "30b1708b-8715-4574-e3dc-87ddd23a1618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To reproduce, try building the following Dockerfile (remove `sudo` if your docker is rootless):\n",
            "```\n",
            "cat <<EOF | sudo BUILDKIT_PROGRESS=plain docker build -\n",
            "FROM library/python:3.11\n",
            "RUN pip install 'uv == 0.3.1' \\\n",
            "    && printf >pyproject.toml '\\\n",
            "      [project]\\n\\\n",
            "      dependencies = [\"django ~= 4.2\"]\\n\\\n",
            "      name = \"demo\"\\n\\\n",
            "      version = \"0.1.0\"\\n\\\n",
            "      requires-python = \">=3.11.7\"\\n\\\n",
            "    '\\\n",
            "    && uv lock\n",
            "RUN uv sync -vv\n",
            "EOF\n",
            "```\n",
            "This is not specific to Django, according to my experiments.\n",
            "The build freezes after `uv_build::run_python_script script=\"get_requires_for_build_editable\", python_version=3.11.9` verbose log, keeping a high CPU load for a few minutes.\n",
            "[DETAILS]\n",
            "However, this only happens if I build this at the root of filesystem. Adding `WORKDIR /home` before installation recovers everything, the build completes in seconds.\n",
            "[DETAILS]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word counts are now subtantially reduced."
      ],
      "metadata": {
        "id": "nXj6xqN70onc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "issues_df.get_column(\"body\").str.split(\" \").list.len().describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "KTlHSfm4LtL6",
        "outputId": "eb727832-94cf-42bb-c6de-6837d9b91581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (9, 2)\n",
              "┌────────────┬───────────┐\n",
              "│ statistic  ┆ value     │\n",
              "│ ---        ┆ ---       │\n",
              "│ str        ┆ f64       │\n",
              "╞════════════╪═══════════╡\n",
              "│ count      ┆ 10313.000 │\n",
              "│ null_count ┆ 0.000     │\n",
              "│ mean       ┆ 81.815    │\n",
              "│ std        ┆ 105.018   │\n",
              "│ min        ┆ 1.000     │\n",
              "│ 25%        ┆ 19.000    │\n",
              "│ 50%        ┆ 51.000    │\n",
              "│ 75%        ┆ 110.000   │\n",
              "│ max        ┆ 3260.000  │\n",
              "└────────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>value</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>10313.000</td></tr><tr><td>&quot;null_count&quot;</td><td>0.000</td></tr><tr><td>&quot;mean&quot;</td><td>81.815</td></tr><tr><td>&quot;std&quot;</td><td>105.018</td></tr><tr><td>&quot;min&quot;</td><td>1.000</td></tr><tr><td>&quot;25%&quot;</td><td>19.000</td></tr><tr><td>&quot;50%&quot;</td><td>51.000</td></tr><tr><td>&quot;75%&quot;</td><td>110.000</td></tr><tr><td>&quot;max&quot;</td><td>3260.000</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comments\n",
        "\n",
        "The comments dataset is processed similarly:\n",
        "Bot comments are removed and long code blocks are snipped."
      ],
      "metadata": {
        "id": "MWpSXDu89aJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df = comments_df.filter(~col(\"user\").str.contains(\"[bot]\", literal=True))"
      ],
      "metadata": {
        "id": "Gpkdf9Fu9Jm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df.get_column(\"body\").str.split(\" \").list.len().describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "NQNhRUcPd8Zy",
        "outputId": "3a51dddf-c47a-47cb-93aa-b0965e59cac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (9, 2)\n",
              "┌────────────┬───────────┐\n",
              "│ statistic  ┆ value     │\n",
              "│ ---        ┆ ---       │\n",
              "│ str        ┆ f64       │\n",
              "╞════════════╪═══════════╡\n",
              "│ count      ┆ 33789.000 │\n",
              "│ null_count ┆ 0.000     │\n",
              "│ mean       ┆ 69.231    │\n",
              "│ std        ┆ 454.410   │\n",
              "│ min        ┆ 1.000     │\n",
              "│ 25%        ┆ 12.000    │\n",
              "│ 50%        ┆ 25.000    │\n",
              "│ 75%        ┆ 55.000    │\n",
              "│ max        ┆ 39896.000 │\n",
              "└────────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>value</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>33789.000</td></tr><tr><td>&quot;null_count&quot;</td><td>0.000</td></tr><tr><td>&quot;mean&quot;</td><td>69.231</td></tr><tr><td>&quot;std&quot;</td><td>454.410</td></tr><tr><td>&quot;min&quot;</td><td>1.000</td></tr><tr><td>&quot;25%&quot;</td><td>12.000</td></tr><tr><td>&quot;50%&quot;</td><td>25.000</td></tr><tr><td>&quot;75%&quot;</td><td>55.000</td></tr><tr><td>&quot;max&quot;</td><td>39896.000</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df = (\n",
        "    comments_df.lazy()\n",
        "    .select(\n",
        "        \"issue_number\",\n",
        "        col(\"html_url\").alias(\"comment_url\"),\n",
        "        (\n",
        "            col(\"body\").str.replace_all(r\"\\s*[\\r\\n]\", \"\\n\")\n",
        "            .str.replace_all(r\"(?s)<details>.*?</details>\", \"[DETAILS]\")\n",
        "            .str.replace_all(r\"(?s)<!--.*?-->\", \"[COMMENT]\")\n",
        "            .str.replace_all(r\"```(?:[^`]|`[^`]|``[^`]){768,}```\", \"[CODE]\")\n",
        "            .str.replace_all(r\"~~~(?:[^~]|~[^~]|~~[^~]){768,}~~~\", \"[CODE]\")\n",
        "            .alias(\"comment_body\")\n",
        "        ),\n",
        "    )\n",
        "    .collect()\n",
        ")"
      ],
      "metadata": {
        "id": "qISeXjCJ9Ow_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df.get_column(\"comment_body\").str.split(\" \").list.len().describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "AVdhLVCtMPxp",
        "outputId": "1da82745-bcd8-4e36-dfc6-56e9189aab86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (9, 2)\n",
              "┌────────────┬───────────┐\n",
              "│ statistic  ┆ value     │\n",
              "│ ---        ┆ ---       │\n",
              "│ str        ┆ f64       │\n",
              "╞════════════╪═══════════╡\n",
              "│ count      ┆ 33789.000 │\n",
              "│ null_count ┆ 0.000     │\n",
              "│ mean       ┆ 43.568    │\n",
              "│ std        ┆ 62.287    │\n",
              "│ min        ┆ 1.000     │\n",
              "│ 25%        ┆ 12.000    │\n",
              "│ 50%        ┆ 24.000    │\n",
              "│ 75%        ┆ 51.000    │\n",
              "│ max        ┆ 1934.000  │\n",
              "└────────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>value</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>33789.000</td></tr><tr><td>&quot;null_count&quot;</td><td>0.000</td></tr><tr><td>&quot;mean&quot;</td><td>43.568</td></tr><tr><td>&quot;std&quot;</td><td>62.287</td></tr><tr><td>&quot;min&quot;</td><td>1.000</td></tr><tr><td>&quot;25%&quot;</td><td>12.000</td></tr><tr><td>&quot;50%&quot;</td><td>24.000</td></tr><tr><td>&quot;75%&quot;</td><td>51.000</td></tr><tr><td>&quot;max&quot;</td><td>1934.000</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joining\n",
        "\n",
        "We are ready to create the corpus.\n",
        "I've considered combining each issue with all associated comments,\n",
        "but that may require an embedding model with a very large context length.\n",
        "Therefore, I use left join to create issue-comment pairs\n",
        "while preserving issues with no comment.\n",
        "\n",
        "URLs are also collected for convenient lookups."
      ],
      "metadata": {
        "id": "VeFga6wQ_ppG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_df = (\n",
        "    issues_df.lazy()\n",
        "    .join(comments_df.lazy(), how=\"left\", left_on=\"number\", right_on=\"issue_number\")\n",
        "    .select(\n",
        "        (\n",
        "            pl.when(col(\"comment_url\").is_null())\n",
        "            .then(col(\"issue_url\"))\n",
        "            .otherwise(\"comment_url\")\n",
        "            .alias(\"url\")\n",
        "        ),\n",
        "        (\n",
        "            pl.when(col(\"comment_body\").is_null())\n",
        "            .then(\n",
        "                pl.format(\"Issue {}: {}\\n\\n{}\", col(\"number\"), col(\"title\"), col(\"body\"))\n",
        "            )\n",
        "            .otherwise(\n",
        "                pl.format(\n",
        "                    \"Issue {}: {}\\n\\n{}\\n\\nComment:\\n{}\",\n",
        "                    col(\"number\"),\n",
        "                    col(\"title\"),\n",
        "                    col(\"body\"),\n",
        "                    col(\"comment_body\"),\n",
        "                )\n",
        "            )\n",
        "            .alias(\"text\")\n",
        "        ),\n",
        "    )\n",
        "    .sort(\"url\")\n",
        "    .collect()\n",
        ")"
      ],
      "metadata": {
        "id": "vGUEcnzOvJ9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert corpus_df.get_column(\"url\").n_unique() == len(corpus_df)"
      ],
      "metadata": {
        "id": "BXp6eRFYF7rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_df.get_column(\"text\").str.split(\" \").list.len().describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "uJxLo6QKMy9S",
        "outputId": "c7c3192c-136a-40f7-8dc3-42eba6d011fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (9, 2)\n",
              "┌────────────┬───────────┐\n",
              "│ statistic  ┆ value     │\n",
              "│ ---        ┆ ---       │\n",
              "│ str        ┆ f64       │\n",
              "╞════════════╪═══════════╡\n",
              "│ count      ┆ 35278.000 │\n",
              "│ null_count ┆ 0.000     │\n",
              "│ mean       ┆ 162.174   │\n",
              "│ std        ┆ 149.027   │\n",
              "│ min        ┆ 3.000     │\n",
              "│ 25%        ┆ 68.000    │\n",
              "│ 50%        ┆ 128.000   │\n",
              "│ 75%        ┆ 213.000   │\n",
              "│ max        ┆ 3312.000  │\n",
              "└────────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>value</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>35278.000</td></tr><tr><td>&quot;null_count&quot;</td><td>0.000</td></tr><tr><td>&quot;mean&quot;</td><td>162.174</td></tr><tr><td>&quot;std&quot;</td><td>149.027</td></tr><tr><td>&quot;min&quot;</td><td>3.000</td></tr><tr><td>&quot;25%&quot;</td><td>68.000</td></tr><tr><td>&quot;50%&quot;</td><td>128.000</td></tr><tr><td>&quot;75%&quot;</td><td>213.000</td></tr><tr><td>&quot;max&quot;</td><td>3312.000</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding issue-comment pairs\n",
        "\n",
        "!!! warning \"&#8203;\"\n",
        "    Running this section likely requires GPU.\n",
        "\n",
        "I pick [`BAAI/bge-m3`][3] as the pretrained embedder.\n",
        "It is based on [`FacebookAI/xlm-roberta-large`][4].\n",
        "It is reasonably sized for a Colab T4 GPU, has a long enough context length of 8192,\n",
        "and is versatile and efficient."
      ],
      "metadata": {
        "id": "bHRy5hj2nfUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = corpus_df.get_column(\"text\").to_list()"
      ],
      "metadata": {
        "id": "Ij-hP0_D_72k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "EMB_CKPT = \"BAAI/bge-m3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMB_CKPT)\n",
        "embedder = AutoModel.from_pretrained(EMB_CKPT)"
      ],
      "metadata": {
        "id": "h4UtKtga_v0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce unncessary padding, the padding length is determined batch by batch.\n",
        "Also, the encoded entries are batched in decreasing lengths,\n",
        "so that the batch maximum length accomodates the entries efficiently.\n",
        "We do have to restore the embeddings to the original order."
      ],
      "metadata": {
        "id": "TFzVJq6OYY4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(\n",
        "    texts: Sequence[str],\n",
        "    *,\n",
        "    tokenizer,\n",
        "    embedder,\n",
        "    batch_size: int,\n",
        "    context_len: int,\n",
        "    device=None,\n",
        "    use_half: bool = True,\n",
        ") -> NDArray:\n",
        "    no_tqdm = len(texts) < batch_size\n",
        "    if device is None:\n",
        "        device = tc.device(\"cuda\" if tc.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    encodings = []\n",
        "    for i in trange(0, len(texts), batch_size, desc=\"Tokenization\", disable=no_tqdm):\n",
        "        # No padding now; pad within each embedder input batch.\n",
        "        batch = tokenizer(\n",
        "            texts[i : i + batch_size], truncation=True, max_length=context_len\n",
        "        )\n",
        "\n",
        "        # dict[list] -> list[dict]\n",
        "        ## Just one way to conform to `tokenizer.pad()`.\n",
        "        encodings.extend(dict(zip(batch, vals)) for vals in zip(*batch.values()))\n",
        "\n",
        "    # Sort by token count in descending order to reduce padding.\n",
        "    # Keep the sorted index to restore the original order later.\n",
        "    ## Reverse view > element-wise negative (https://stackoverflow.com/a/16486305)\n",
        "    sorted_index = np.argsort([len(x[\"input_ids\"]) for x in encodings])[::-1]\n",
        "    encodings = [encodings[i] for i in sorted_index]\n",
        "\n",
        "    embedder = embedder.to(device).eval()\n",
        "    # Using float16 only on GPU.\n",
        "    if device.type == \"cuda\" and use_half:\n",
        "        embedder = embedder.half()\n",
        "\n",
        "    embeddings = []\n",
        "    with tc.inference_mode():\n",
        "        for i in trange(\n",
        "            0,\n",
        "            len(encodings),\n",
        "            batch_size,\n",
        "            desc=\"Embedding\",\n",
        "            disable=len(encodings) < batch_size,\n",
        "        ):\n",
        "            # Within-batch padding\n",
        "            ## `BatchEncoding` has method `to()`.\n",
        "            padded = tokenizer.pad(\n",
        "                encodings[i : i + batch_size],\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(device)\n",
        "\n",
        "            # [CLS] pooling with normalization\n",
        "            embeddings.append(\n",
        "                F.normalize(embedder(**padded).last_hidden_state[:, 0], dim=-1)\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "\n",
        "    # Merge, cast to float32 (for Faiss), and restore original order.\n",
        "    return np.concatenate(embeddings, 0, dtype=np.float32)[np.argsort(sorted_index)]"
      ],
      "metadata": {
        "id": "gZv5Dg06_1k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding process takes about 10 minutes in a Colab T4 GPU runtime."
      ],
      "metadata": {
        "id": "hyiP3FyERD26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embed(\n",
        "    corpus,\n",
        "    tokenizer=tokenizer,\n",
        "    embedder=embedder,\n",
        "    batch_size=32,\n",
        "    context_len=4096,\n",
        "    use_half=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "3f26888d485f46a39e7cddbb25dfaa80",
            "4042d9873096405ea9e1fb756fb45881",
            "4968b789be5446c5bdc6215588e22b59",
            "d8698199c70d49d18c1b941b4b409df6",
            "f1140c21392c4609a375954eecf2c900",
            "467084172407441eafab461f15c345cb",
            "aceb2aa98c2e4180bca382d5634e3e77",
            "fa7fdcda90b74a569de78543ca3e27ca",
            "9c48254b2dcf427390ad727db833672d",
            "23f61203ecd44809a135580449d784e9",
            "a1f517a437f746f88743456ca9662c73",
            "c91b16d8635447fe94728093a6c8ef27",
            "bcc6c47dac7043fd9dc49396c8c29ed7",
            "415debc483964f65a560c874965f5632",
            "33a196f5f6aa4b7591b080bb5262a603",
            "223c803c2635402ebce8b410bb5f35a1",
            "39bc726f542e452c91a05e80b2544029",
            "3eaae8a2e55f46c3886f7f5be1bbec60",
            "35cb0a27e0014315bf390722ef21ad41",
            "fc2c46a009e440cf98fef60510909137",
            "979f3ec960b14eaba300b71e34953f52",
            "6d4d79f70f944d40ba6911c8c2ab2af9"
          ]
        },
        "id": "MqC0T9BAO6zx",
        "outputId": "1e256f5a-9beb-442d-e5ed-d34835c58abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenization:   0%|          | 0/1103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f26888d485f46a39e7cddbb25dfaa80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Embedding:   0%|          | 0/1103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c91b16d8635447fe94728093a6c8ef27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rJWUdl5nona",
        "outputId": "48250091-dec6-45b6-cf8c-d854b20bc0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35278, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create some example user questions about `astral-sh/uv` as queries."
      ],
      "metadata": {
        "id": "TzgsqigIc-Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"What is the difference between `uv pip install` and `uv add`?\",\n",
        "    \"How to update Python in my venv to the latest version?\",\n",
        "    \"How to install the CPU version of PyTorch?\",\n",
        "    \"Can I add a package dependency without version requirement?\",\n",
        "    \"What does the `.python-version` file do?\",\n",
        "]"
      ],
      "metadata": {
        "id": "fjKvQ3XrA9Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_embeddings = embed(\n",
        "    queries,\n",
        "    tokenizer=tokenizer,\n",
        "    embedder=embedder,\n",
        "    batch_size=8,\n",
        "    context_len=512,\n",
        "    use_half=True,\n",
        ")"
      ],
      "metadata": {
        "id": "mseaiLPvFyHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit (q_embeddings @ embeddings.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0bg3THqF0d8",
        "outputId": "3f2a7e94-cbe4-4abe-d5bb-a0f52d39efbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55.6 ms ± 1.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The higher the inner product of a query embedding and an issue-comment pair embedding,\n",
        "the more similar they should be,\n",
        "and the more likely the issue-comment pair contains an answer to the question."
      ],
      "metadata": {
        "id": "CQJBm7i_dcEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_indexes = (q_embeddings @ embeddings.T).argmax(-1).tolist()"
      ],
      "metadata": {
        "id": "DfVvLlUNGIm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_query_results(queries, corpus_df, indexes):\n",
        "    for query, i in zip(queries, indexes):\n",
        "        print(f\"Query: {query}\\n\")\n",
        "        print(f\"Result URL: {corpus_df.get_column('url')[i]}\\n\")\n",
        "        print(f\"Result:\\n{corpus_df.get_column('text')[i]}\\n\")\n",
        "        print(f\"==================================================\\n\")"
      ],
      "metadata": {
        "id": "rr61WpZHGXji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though only some of the comments directly answer the questions,\n",
        "all the retrieved issues are indeed highly relevant."
      ],
      "metadata": {
        "id": "W_x78vbTefBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_query_results(queries, corpus_df, result_indexes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDLAfnUjRlhQ",
        "outputId": "21f56203-d12b-44b2-9b8b-56c4fbdf8d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the difference between `uv pip install` and `uv add`?\n",
            "\n",
            "Result URL: https://github.com/astral-sh/uv/issues/9219#issuecomment-2613016513\n",
            "\n",
            "Result:\n",
            "Issue 9219: What's the difference between `uv pip install` and `uv add`\n",
            "\n",
            "[COMMENT]\n",
            "I've been using `uv` for a while and I really enjoy it. I keep stumbling onto the same confusion though. I never quite know whether do to:\n",
            "```sh\n",
            "uv init\n",
            "uv venv\n",
            "uv add polars marimo\n",
            "uv run hello.py\n",
            "```\n",
            "or\n",
            "```sh\n",
            "uv init\n",
            "uv venv\n",
            "source .venv/bin/activate\n",
            "pip install polars marimo\n",
            "python hello.py\n",
            "```\n",
            "are these two above equivalent?\n",
            "---\n",
            "also are these two equivalent?\n",
            "```\n",
            "uv add polars\n",
            "```\n",
            "```\n",
            "uv pip install polars\n",
            "```\n",
            "\n",
            "Comment:\n",
            "1. `pip install` will only install into its own environment, `uv pip install` can target other environments.\n",
            "2. `uv run script.py` will activate a virtual environment if necessary, and read PEP 723 inline metadata or sync your project if necessary, then call `python script.py` — the latter just uses your current environment as-is.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Query: How to update Python in my venv to the latest version?\n",
            "\n",
            "Result URL: https://github.com/astral-sh/uv/issues/11317#issuecomment-2643263625\n",
            "\n",
            "Result:\n",
            "Issue 11317: How to update `uv` managed Python version for applications?\n",
            "\n",
            "### Question\n",
            "I have Python installed via uv's `uv python install 3.x`. The project is an project/application (so not a library).\n",
            "Let's say there is a new Python (standalone) release, e.g.\n",
            "3.12.9 -> 3.13.2\n",
            "3.13.1 -> 3.13.2\n",
            "It there a way to 'update' the standalone Python from uv, also replacing the existing `.venv`?\n",
            "My current solution is to install the new python with `uv python install 3.x`, deleting the .venv and recreating it with the target python version (and delete the `.python-version` file). This works fine, but is a rather tedious process.\n",
            "I would love to have a project scoped command like `uv sync --update-python-to 3.13.2`  (handling also the installation of the standalone-python).\n",
            "Similar possibilities:\n",
            "- `uv python update` (either updating existing standalone python versions on patch level AND/OR updating project .venv on python version patch level)\n",
            "- `uv python update 3.13` (updating project .venv to latest specified python version)\n",
            "I guess there are smarter persons, having better ideas how to handle these commands. Is there something in uv right now, that I'm misssing?\n",
            "### Platform\n",
            "_No response_\n",
            "### Version\n",
            "0.5.29\n",
            "\n",
            "Comment:\n",
            "Dang, didn't found that one. Thx!\n",
            "\n",
            "==================================================\n",
            "\n",
            "Query: How to install the CPU version of PyTorch?\n",
            "\n",
            "Result URL: https://github.com/astral-sh/uv/issues/11079#issuecomment-2624678978\n",
            "\n",
            "Result:\n",
            "Issue 11079: Add pytorch documentation section on how to install for Intel GPUs\n",
            "\n",
            "### Summary\n",
            "Pytorch 2.6 [adds support for Intel GPUs](https://pytorch.org/docs/main/notes/get_start_xpu.html). The current [uv pytorch install docs](https://docs.astral.sh/uv/guides/integration/pytorch/#using-a-pytorch-index) include instructions for:\n",
            "- CPU only\n",
            "- Various CUDA versions\n",
            "- ROCm\n",
            "But not yet for Intel GPUs.\n",
            "My current attempt at mimicking the docs for existing GPUs is\n",
            "```pyproject.toml\n",
            "# pyproject.toml\n",
            "[project]\n",
            "name = \"pytorch-intel\"\n",
            "version = \"0.1.0\"\n",
            "description = \"Add your description here\"\n",
            "readme = \"README.md\"\n",
            "requires-python = \">=3.12\"\n",
            "dependencies = [\"torch>=2.6.0\"]\n",
            "[[tool.uv.index]]\n",
            "name = \"pytorch-intel-gpu\"\n",
            "url = \"https://download.pytorch.org/whl/xpu\"\n",
            "explicit = true\n",
            "[tool.uv.sources]\n",
            "torch = [{ index = \"pytorch-intel-gpu\", marker = \"platform_system == 'Linux'\" }]\n",
            "```\n",
            "```python\n",
            "# hello.py\n",
            "import torch\n",
            "print(torch.xpu.is_available())\n",
            "```\n",
            "Running `uv run hello.py` produces\n",
            "```sh\n",
            "  × No solution found when resolving dependencies for split (sys_platform\n",
            "  │ == 'linux'):\n",
            "  ╰─▶ Because there is no version of pytorch-triton-xpu{platform_machine\n",
            "      == 'x86_64' and sys_platform == 'linux'}==3.2.0 and torch==2.6.0+xpu\n",
            "      depends on pytorch-triton-xpu{platform_machine == 'x86_64'\n",
            "      and sys_platform == 'linux'}==3.2.0, we can conclude that\n",
            "      torch==2.6.0+xpu cannot be used.\n",
            "      And because only the following versions of torch{sys_platform ==\n",
            "      'linux'} are available:\n",
            "          torch{sys_platform == 'linux'}<2.6.0\n",
            "          torch{sys_platform == 'linux'}==2.6.0+xpu\n",
            "      and your project depends on torch{sys_platform == 'linux'}>=2.6.0, we\n",
            "      can conclude that your project's requirements are unsatisfiable.\n",
            "```\n",
            "### Example\n",
            "_No response_\n",
            "\n",
            "Comment:\n",
            "Yes, this worked for me!\n",
            "I got a warning from pytorch that it couldn't initialize numpy. Adding numpy to the pyproject fixed the warning. \n",
            "\n",
            "==================================================\n",
            "\n",
            "Query: Can I add a package dependency without version requirement?\n",
            "\n",
            "Result URL: https://github.com/astral-sh/uv/issues/6476#issuecomment-2305937941\n",
            "\n",
            "Result:\n",
            "Issue 6476: Allow adding a dependency with no version constraint\n",
            "\n",
            "`uv add` adds a lower bound version constraint by default. For example, calling `uv add requests` currently adds the dependency `\"requests>=2.32.3\"` to `pyproject.toml`.\n",
            "It's possible to have uv use different upper and/or lower bounds, but I could not find a way to add a dependency without any version constraint at all. For example, I want the dependency `\"requests\"` added to `pyproject.toml` without any bounds.\n",
            "The current default behavior is fine, but I think there should be some way to add an unconstrained dependency, either with a global configuration setting that changes the default behavior, or with a command-line option for `uv add`.\n",
            "Apologies if I missed something in the documentation and this is already possible. (Of course I could edit `pyproject.toml` manually, but it doesn't seem like that should be necessary.)\n",
            "Thanks for the great work on a fantastic project!\n",
            "\n",
            "Comment:\n",
            "That seems ok to me.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Query: What does the `.python-version` file do?\n",
            "\n",
            "Result URL: https://github.com/astral-sh/uv/issues/8920#issuecomment-2465005344\n",
            "\n",
            "Result:\n",
            "Issue 8920: Purpose of .python-version?\n",
            "\n",
            "[COMMENT]\n",
            "Currently on `uv init` a project boilerplate created includes the file `.python-version`.\n",
            "What is the purpose of this file, if there are constraints in `pyproject.toml` and `uv.lock`?\n",
            "Is it just for compatibility with tools like pyenv or is there more to it?\n",
            "I really like how clean project directory became after moving a lot of stuff (like `requirements.*`) to `pyproject.toml`. So wonder, if `.python-version` is really required in two senses:\n",
            "1. Is it required for the projects now? I tested by deleting `.python-version`, and everything works as expected and this file is not recreated with commands like `uv run`, `uv sync` and `uv lock`\n",
            "2. Since a lot of constraints/pinned versions of things are now in `pyproject.toml` and `uv.lock`, maybe if `.python-version` still plays some important role, its function can be moved to `pyproject.toml` or `uv.lock` in future?\n",
            "Asking as a 5-year user of `pyenv` and `.python-version`. They are great, but I don't really miss them.\n",
            "Also, couldn't find peps related to this file. \n",
            "\n",
            "Comment:\n",
            "Related - https://github.com/astral-sh/uv/issues/8247\n",
            "A `.python-version` file is not strictly required, but it's useful to have it when developing a project as it allows you to specify the exact Python version you are using to do development. It's different to the `requires-python` field  - that is the range of Python versions supported by your project.\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Faiss index to improve query speed\n",
        "\n",
        "[Faiss][5] is a library that can create indexes to speed up similarity search\n",
        "among dense vector embeddings, often at very little cost of accuracy.\n",
        "\n",
        "I use an inverted file index with inner product as metric.\n",
        "The `nlist` is the number of partitions made (in the form of inverted lists)\n",
        "in the embedding space,\n",
        "and the `nprob` is the number of partitions examined per query.\n",
        "They are set roughly according to the [guideilnes][6]."
      ],
      "metadata": {
        "id": "zPZtMlX7A_BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = embeddings.shape[-1]\n",
        "nlist = 2048\n",
        "nprob = 16"
      ],
      "metadata": {
        "id": "vmETXU0_I0Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantizer = faiss.IndexFlatIP(D)\n",
        "faiss_index = faiss.IndexIVFFlat(quantizer, D, nlist, faiss.METRIC_INNER_PRODUCT)"
      ],
      "metadata": {
        "id": "CoUN0HEkJhgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the index takes less than 1 minute."
      ],
      "metadata": {
        "id": "5EHlqDpjjMZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_index.train(embeddings)\n",
        "faiss_index.add(embeddings)"
      ],
      "metadata": {
        "id": "IVZHnGI8JuV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_index.is_trained, faiss_index.ntotal, faiss_index.nprobe"
      ],
      "metadata": {
        "id": "viI2SZL7uiHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a339f73-530d-4c2e-cc5c-c97debab8e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 35278, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_index.nprobe = nprob"
      ],
      "metadata": {
        "id": "caMGQ3CxqxpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The index improves query time by a factor of $\\sim 20$,\n",
        "and happens to get exactly the same results from our example queries."
      ],
      "metadata": {
        "id": "NMjKVmy2jZav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit faiss_index.search(q_embeddings, k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Sv50kViSfqx",
        "outputId": "6028099b-9f39-4914-d463-8c499553a12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.71 ms ± 69 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics, faiss_result_indexes = faiss_index.search(q_embeddings, k=1)\n",
        "result_indexes, faiss_result_indexes.reshape(-1).tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV9gqy1TSkq_",
        "outputId": "76fafa7b-deaf-4acd-e587-408409bab192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([25352, 2833, 2230, 18026, 24623], [25352, 2833, 2230, 18026, 24623])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a Reranker to improve query results"
      ],
      "metadata": {
        "id": "cZ-b17U5KzOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A reranker is supposed to evaluate relevance between embeddings\n",
        "more accurately but slowly than distance metrics.\n",
        "\n",
        "Here I use [`BAAI/bge-reranker-v2-m3`][7], the matching reranker of `BAAI/bge-m3`."
      ],
      "metadata": {
        "id": "V7P5QzzykQEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "RRK_CKPT = \"BAAI/bge-reranker-v2-m3\"\n",
        "rerank_tokenizer = AutoTokenizer.from_pretrained(RRK_CKPT)\n",
        "reranker = AutoModelForSequenceClassification.from_pretrained(RRK_CKPT)"
      ],
      "metadata": {
        "id": "IIZUbF0iMhTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank(\n",
        "    query: str,\n",
        "    corpus: Sequence[str],\n",
        "    indexes: Sequence[int],\n",
        "    tokenizer,\n",
        "    reranker,\n",
        "    context_len: int,\n",
        "    device=None,\n",
        "):\n",
        "    if device is None:\n",
        "        device = tc.device(\"cuda\" if tc.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    pairs = [[query, corpus[i]] for i in indexes]\n",
        "    inputs = tokenizer(\n",
        "        pairs, padding=True, truncation=True, max_length=context_len, return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    reranker = reranker.to(device).eval()\n",
        "    with tc.inference_mode():\n",
        "        logits = reranker(**inputs).logits.reshape(-1).cpu().numpy()\n",
        "\n",
        "    return np.array(indexes)[(-logits).argsort()]"
      ],
      "metadata": {
        "id": "ykZVHGAFM1Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, issue-comment pairs of top-10 similarities are retrieved from the Faiss index.\n",
        "Then, the reranker reorders them in decreasing scores."
      ],
      "metadata": {
        "id": "mMSgJyMTluHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics, faiss_result_indexes = faiss_index.search(q_embeddings, k=10)"
      ],
      "metadata": {
        "id": "E5XRK6dxTGwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reranked_indexes = np.array(\n",
        "    [\n",
        "        rerank(q, corpus, i, rerank_tokenizer, reranker, context_len=4096)\n",
        "        for q, i in zip(queries, faiss_result_indexes)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "0Kje0Tv0eVXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our examples, the final top-1 results are quite similar.\n",
        "But the reranked results for question 1 and 3 are arguably more complete."
      ],
      "metadata": {
        "id": "HyKz2X0DnR2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_result_indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mK4ZW8ohJxP",
        "outputId": "675941d4-6a36-4085-c1a9-86e156aabc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[25352, 25339, 25344, 25340, 25351, 25350, 25345, 25343, 25346,\n",
              "        25341],\n",
              "       [ 2833,  2832, 22932, 25560, 20466, 25559, 20467, 20468, 20469,\n",
              "        20464],\n",
              "       [ 2230,  2231,  6440,  6458,  2229,  6457,  6455, 16575,  6453,\n",
              "         6437],\n",
              "       [18026, 18024, 18025, 23750,  4664, 12986, 13010, 16315,  4665,\n",
              "        13004],\n",
              "       [24623, 24624, 33204,  8035,  4781, 25094, 20774,  8034,  3762,\n",
              "        15749]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reranked_indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6vTgI7wgw8z",
        "outputId": "3e28c1a8-5c15-4972-c71b-1e2215237b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[25339, 25352, 25340, 25344, 25341, 25350, 25351, 25346, 25345,\n",
              "        25343],\n",
              "       [ 2833,  2832, 25559, 20466, 25560, 20467, 20464, 22932, 20468,\n",
              "        20469],\n",
              "       [ 6458,  6440,  6453,  6457,  6455,  6437,  2229,  2230,  2231,\n",
              "        16575],\n",
              "       [18026, 18024, 18025,  4665,  4664, 23750, 16315, 12986, 13010,\n",
              "        13004],\n",
              "       [24623, 25094, 15749, 24624,  4781,  8034,  3762,  8035, 33204,\n",
              "        20774]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_query_results(queries, corpus_df, reranked_indexes[:, 0].reshape(-1).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "597i_5ysgz9v",
        "outputId": "7eaf201f-8a87-40a9-ceb8-9c9e68f074b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the difference between `uv pip install` and `uv add`?\n",
            "\n",
            "Result URL: https://github.com/astral-sh/uv/issues/9219#issuecomment-2485573603\n",
            "\n",
            "Result:\n",
            "Issue 9219: What's the difference between `uv pip install` and `uv add`\n",
            "\n",
            "[COMMENT]\n",
            "I've been using `uv` for a while and I really enjoy it. I keep stumbling onto the same confusion though. I never quite know whether do to:\n",
            "```sh\n",
            "uv init\n",
            "uv venv\n",
            "uv add polars marimo\n",
            "uv run hello.py\n",
            "```\n",
            "or\n",
            "```sh\n",
            "uv init\n",
            "uv venv\n",
            "source .venv/bin/activate\n",
            "pip install polars marimo\n",
            "python hello.py\n",
            "```\n",
            "are these two above equivalent?\n",
            "---\n",
            "also are these two equivalent?\n",
            "```\n",
            "uv add polars\n",
            "```\n",
            "```\n",
            "uv pip install polars\n",
            "```\n",
            "\n",
            "Comment:\n",
            "``uv add`` choose universal or cross-platform dependencies , and ``uv add`` is a project API.\n",
            "https://docs.astral.sh/uv/concepts/projects/\n",
            "This is my understanding, but the more correct interpretation should be based on the documentation and the uv team's explanation.\n",
            "> Suppose a dependency has versions 1.0.0 and 1.1.0 on Windows, but versions 1.0.0, 1.1.0, and 1.2.0 on Linux. If you're using uv on Linux, uv pip install would typically install the latest version (1.2.0), while uv add would select version 1.1.0, ensuring compatibility across Windows and Linux.\n",
            "The explanation in the document should be here:\n",
            "* uv add\n",
            "> uv's lockfile (uv.lock) is created with a universal resolution and is portable across platforms. This ensures that dependencies are locked for everyone working on the project, regardless of operating system, architecture, and Python version. The uv lockfile is created and modified by project commands such as uv lock, uv sync, and uv add.\n",
            "https://docs.astral.sh/uv/concepts/resolution/#universal-resolution\n",
            "* uv pip install\n",
            "> By default, uv tries to use the latest version of each package. For example, uv pip install flask>=2.0.0 will install the latest version of Flask, e.g., 3.0.0. If flask>=2.0.0 is a dependency of the project, only flask 3.0.0 will be used. This is important, for example, because running tests will not check that the project is actually compatible with its stated lower bound of flask 2.0.0.\n",
            "https://docs.astral.sh/uv/concepts/resolution/#resolution-strategy\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "Query: How to update Python in my venv to the latest version?\n",
            "\n",
            "Result URL: https://github.com/astral-sh/uv/issues/11317#issuecomment-2643263625\n",
            "\n",
            "Result:\n",
            "Issue 11317: How to update `uv` managed Python version for applications?\n",
            "\n",
            "### Question\n",
            "I have Python installed via uv's `uv python install 3.x`. The project is an project/application (so not a library).\n",
            "Let's say there is a new Python (standalone) release, e.g.\n",
            "3.12.9 -> 3.13.2\n",
            "3.13.1 -> 3.13.2\n",
            "It there a way to 'update' the standalone Python from uv, also replacing the existing `.venv`?\n",
            "My current solution is to install the new python with `uv python install 3.x`, deleting the .venv and recreating it with the target python version (and delete the `.python-version` file). This works fine, but is a rather tedious process.\n",
            "I would love to have a project scoped command like `uv sync --update-python-to 3.13.2`  (handling also the installation of the standalone-python).\n",
            "Similar possibilities:\n",
            "- `uv python update` (either updating existing standalone python versions on patch level AND/OR updating project .venv on python version patch level)\n",
            "- `uv python update 3.13` (updating project .venv to latest specified python version)\n",
            "I guess there are smarter persons, having better ideas how to handle these commands. Is there something in uv right now, that I'm misssing?\n",
            "### Platform\n",
            "_No response_\n",
            "### Version\n",
            "0.5.29\n",
            "\n",
            "Comment:\n",
            "Dang, didn't found that one. Thx!\n",
            "\n",
            "==================================================\n",
            "\n",
            "Query: How to install the CPU version of PyTorch?\n",
            "\n",
            "Result URL: https://github.com/astral-sh/uv/issues/1497#issuecomment-2102236399\n",
            "\n",
            "Result:\n",
            "Issue 1497: Cannot install the CPU version of torch\n",
            "\n",
            "I tried to install the CPU version of torch but could not.\n",
            "```bash\n",
            "uv pip install torch==2.1.0+cpu --find-links https://download.pytorch.org/whl/torch_stable.html\n",
            "# and next command gives the same result.\n",
            "uv pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cpu\n",
            "```\n",
            "```log\n",
            "  × No solution found when resolving dependencies:\n",
            "  ╰─▶ Because there is no version of torch==2.1.0+cpu and you require torch==2.1.0+cpu, we can conclude that the requirements are unsatisfiable.\n",
            "```\n",
            "uv version: v0.1.2\n",
            "Python 3.11.7\n",
            "Ubuntu 20.4\n",
            "X86_64 Architecture\n",
            "---\n",
            "By the way, the install of the GPU version of torch is successful.\n",
            "```bash\n",
            "uv pip install torch==2.1.0\n",
            "# success\n",
            "```\n",
            "\n",
            "Comment:\n",
            "Perhaps it's due to a lack of supported variants that they omitted the local identifier there? (_[here upstream PyTorch states they don't intend to publish `aarch64` variants with GPU accel](https://github.com/pytorch/pytorch/issues/110791#issuecomment-1753315240)_)\n",
            "```bash\n",
            "# Torch 2.3.0 + Python 3.12 Linux x86_64/aarch64 wheels\n",
            "# +cpu\n",
            "# https://download.pytorch.org/whl/cpu/torch/\n",
            "torch-2.3.0+cpu-cp312-cp312-linux_x86_64.whl\n",
            "# No local identifier for arm64:\n",
            "torch-2.3.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n",
            "# +cu121\n",
            "# https://download.pytorch.org/whl/cu121/torch/\n",
            "torch-2.3.0+cu121-cp312-cp312-linux_x86_64.whl\n",
            "```\n",
            "## Reference\n",
            "Using Docker from an AMD64 machine to test ARM64 environment:\n",
            "```console\n",
            "# Base environment:\n",
            "$ arch\n",
            "x86_64\n",
            "$ docker run --rm -it --platform linux/arm64 --workdir /tmp fedora:40 bash\n",
            "$ arch\n",
            "aarch64\n",
            "# Install uv:\n",
            "$ curl -LsSf https://astral.sh/uv/install.sh | sh && source $HOME/.cargo/env\n",
            "# Create and enter venv:\n",
            "$ uv venv && source .venv/bin/activate\n",
            "# Install:\n",
            "$ uv pip install \\\n",
            "    --extra-index-url https://download.pytorch.org/whl/cpu \\\n",
            "    torch torchvision torchaudio\n",
            "Resolved 13 packages in 20.18s\n",
            "Downloaded 13 packages in 12.26s\n",
            "Installed 13 packages in 486ms\n",
            " + filelock==3.13.1\n",
            " + fsspec==2024.2.0\n",
            " + jinja2==3.1.3\n",
            " + markupsafe==2.1.5\n",
            " + mpmath==1.3.0\n",
            " + networkx==3.2.1\n",
            " + numpy==1.26.3\n",
            " + pillow==10.2.0\n",
            " + sympy==1.12\n",
            " + torch==2.3.0\n",
            " + torchaudio==2.3.0\n",
            " + torchvision==0.18.0\n",
            " + typing-extensions==4.9.0\n",
            "```\n",
            "---\n",
            "You can kind of see why PyTorch has local identifier variants when comparing to PyPi?:\n",
            "```console\n",
            "# ARM64 (PyPi instead of PyTorch)\n",
            "# Slightly newer versions of the packages but otherwise equivalent\n",
            "$ uv pip install torch torchvision torchaudio\n",
            "Resolved 13 packages in 1.08s\n",
            "Downloaded 13 packages in 23.73s\n",
            "Installed 13 packages in 425ms\n",
            " + filelock==3.14.0\n",
            " + fsspec==2024.3.1\n",
            " + jinja2==3.1.4\n",
            " + markupsafe==2.1.5\n",
            " + mpmath==1.3.0\n",
            " + networkx==3.3\n",
            " + numpy==1.26.4\n",
            " + pillow==10.3.0\n",
            " + sympy==1.12\n",
            " + torch==2.3.0\n",
            " + torchaudio==2.3.0\n",
            " + torchvision==0.18.0\n",
            " + typing-extensions==4.11.0\n",
            "```\n",
            "[CODE]\n",
            "---\n",
            "## Resolve platform packaging inconsistency upstream\n",
            "Maybe open an issue about it upstream since it seems to be more of a packaging concern unrelated to `uv`?:\n",
            "https://github.com/pytorch/pytorch/issues/110791#issuecomment-1753334468\n",
            "> _please do not hesitate to open a new issue (or a PR) if you want to propose dropping `+cpu` suffix for Linux and Windows wheels, as nobody seems to be relying on the old installation method anyway._\n",
            "Which makes sense that CPU should be the default, with local identifiers only used for actual variants (_eg: different cuda version support_). Might introduce some friction to any existing CI perhaps, but you can see similar with nvidia's own Docker images they publish where the naming convention isn't always consistent 😅 (_at least it broke a PyTorch docker build CI task recently_)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Query: Can I add a package dependency without version requirement?\n",
            "\n",
            "Result URL: https://github.com/astral-sh/uv/issues/6476#issuecomment-2305937941\n",
            "\n",
            "Result:\n",
            "Issue 6476: Allow adding a dependency with no version constraint\n",
            "\n",
            "`uv add` adds a lower bound version constraint by default. For example, calling `uv add requests` currently adds the dependency `\"requests>=2.32.3\"` to `pyproject.toml`.\n",
            "It's possible to have uv use different upper and/or lower bounds, but I could not find a way to add a dependency without any version constraint at all. For example, I want the dependency `\"requests\"` added to `pyproject.toml` without any bounds.\n",
            "The current default behavior is fine, but I think there should be some way to add an unconstrained dependency, either with a global configuration setting that changes the default behavior, or with a command-line option for `uv add`.\n",
            "Apologies if I missed something in the documentation and this is already possible. (Of course I could edit `pyproject.toml` manually, but it doesn't seem like that should be necessary.)\n",
            "Thanks for the great work on a fantastic project!\n",
            "\n",
            "Comment:\n",
            "That seems ok to me.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Query: What does the `.python-version` file do?\n",
            "\n",
            "Result URL: https://github.com/astral-sh/uv/issues/8920#issuecomment-2465005344\n",
            "\n",
            "Result:\n",
            "Issue 8920: Purpose of .python-version?\n",
            "\n",
            "[COMMENT]\n",
            "Currently on `uv init` a project boilerplate created includes the file `.python-version`.\n",
            "What is the purpose of this file, if there are constraints in `pyproject.toml` and `uv.lock`?\n",
            "Is it just for compatibility with tools like pyenv or is there more to it?\n",
            "I really like how clean project directory became after moving a lot of stuff (like `requirements.*`) to `pyproject.toml`. So wonder, if `.python-version` is really required in two senses:\n",
            "1. Is it required for the projects now? I tested by deleting `.python-version`, and everything works as expected and this file is not recreated with commands like `uv run`, `uv sync` and `uv lock`\n",
            "2. Since a lot of constraints/pinned versions of things are now in `pyproject.toml` and `uv.lock`, maybe if `.python-version` still plays some important role, its function can be moved to `pyproject.toml` or `uv.lock` in future?\n",
            "Asking as a 5-year user of `pyenv` and `.python-version`. They are great, but I don't really miss them.\n",
            "Also, couldn't find peps related to this file. \n",
            "\n",
            "Comment:\n",
            "Related - https://github.com/astral-sh/uv/issues/8247\n",
            "A `.python-version` file is not strictly required, but it's useful to have it when developing a project as it allows you to specify the exact Python version you are using to do development. It's different to the `requires-python` field  - that is the range of Python versions supported by your project.\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1]: https://huggingface.co/learn/nlp-course/en/chapter5/5\n",
        "[2]: make-huggingface-dataset-of-github-repo-issues.md\n",
        "[3]: https://huggingface.co/BAAI/bge-m3\n",
        "[4]: https://huggingface.co/FacebookAI/xlm-roberta-large\n",
        "[5]: https://github.com/facebookresearch/faiss\n",
        "[6]: https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index#if-below-1m-vectors-ivfk\n",
        "[7]: https://huggingface.co/BAAI/bge-reranker-v2-m3"
      ],
      "metadata": {
        "id": "ctuTghxm-nXU"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3f26888d485f46a39e7cddbb25dfaa80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4042d9873096405ea9e1fb756fb45881",
              "IPY_MODEL_4968b789be5446c5bdc6215588e22b59",
              "IPY_MODEL_d8698199c70d49d18c1b941b4b409df6"
            ],
            "layout": "IPY_MODEL_f1140c21392c4609a375954eecf2c900"
          }
        },
        "4042d9873096405ea9e1fb756fb45881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_467084172407441eafab461f15c345cb",
            "placeholder": "​",
            "style": "IPY_MODEL_aceb2aa98c2e4180bca382d5634e3e77",
            "value": "Tokenization: 100%"
          }
        },
        "4968b789be5446c5bdc6215588e22b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa7fdcda90b74a569de78543ca3e27ca",
            "max": 1103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c48254b2dcf427390ad727db833672d",
            "value": 1103
          }
        },
        "d8698199c70d49d18c1b941b4b409df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23f61203ecd44809a135580449d784e9",
            "placeholder": "​",
            "style": "IPY_MODEL_a1f517a437f746f88743456ca9662c73",
            "value": " 1103/1103 [00:28&lt;00:00, 53.78it/s]"
          }
        },
        "f1140c21392c4609a375954eecf2c900": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467084172407441eafab461f15c345cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aceb2aa98c2e4180bca382d5634e3e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa7fdcda90b74a569de78543ca3e27ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c48254b2dcf427390ad727db833672d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23f61203ecd44809a135580449d784e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f517a437f746f88743456ca9662c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c91b16d8635447fe94728093a6c8ef27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcc6c47dac7043fd9dc49396c8c29ed7",
              "IPY_MODEL_415debc483964f65a560c874965f5632",
              "IPY_MODEL_33a196f5f6aa4b7591b080bb5262a603"
            ],
            "layout": "IPY_MODEL_223c803c2635402ebce8b410bb5f35a1"
          }
        },
        "bcc6c47dac7043fd9dc49396c8c29ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39bc726f542e452c91a05e80b2544029",
            "placeholder": "​",
            "style": "IPY_MODEL_3eaae8a2e55f46c3886f7f5be1bbec60",
            "value": "Embedding: 100%"
          }
        },
        "415debc483964f65a560c874965f5632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35cb0a27e0014315bf390722ef21ad41",
            "max": 1103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc2c46a009e440cf98fef60510909137",
            "value": 1103
          }
        },
        "33a196f5f6aa4b7591b080bb5262a603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_979f3ec960b14eaba300b71e34953f52",
            "placeholder": "​",
            "style": "IPY_MODEL_6d4d79f70f944d40ba6911c8c2ab2af9",
            "value": " 1103/1103 [08:48&lt;00:00, 27.51it/s]"
          }
        },
        "223c803c2635402ebce8b410bb5f35a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39bc726f542e452c91a05e80b2544029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eaae8a2e55f46c3886f7f5be1bbec60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35cb0a27e0014315bf390722ef21ad41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc2c46a009e440cf98fef60510909137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "979f3ec960b14eaba300b71e34953f52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d4d79f70f944d40ba6911c8c2ab2af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}